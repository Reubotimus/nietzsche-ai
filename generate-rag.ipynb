{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU \"langchain[openai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "if not os.environ.get(\"NEON_API_KEY\"):\n",
    "  os.environ[\"NEON_API_KEY\"] = getpass.getpass(\"Enter API key for Neon: \")\n",
    "\n",
    "\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres import PGVector\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=\"my_docs\",\n",
    "    connection=\"postgresql+psycopg://neondb_owner:npg_NkGRWx4yetz3@ep-odd-water-a7dn9f9o-pooler.ap-southeast-2.aws.neon.tech/neondb?sslmode=require\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "\n",
    "# point DirectoryLoader at your folder of PDFs\n",
    "loader = DirectoryLoader(\n",
    "    \"./sources/primary\",            # folder containing your PDFs\n",
    "    glob=\"**/*.pdf\",                # recursive glob to pick up all .pdf files\n",
    "    loader_cls=PyPDFLoader,         # PDF‐specific loader\n",
    ")\n",
    "\n",
    "# this will load every PDF as one or more Document objects\n",
    "docs = loader.load()\n",
    "\n",
    "# sanity check: how many docs and total characters?\n",
    "total_chars = sum(len(doc.page_content) for doc in docs)\n",
    "print(f\"Loaded {len(docs)} document chunks\")\n",
    "print(f\"Total characters across all documents: {total_chars}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_loader = DirectoryLoader(\n",
    "    \"./sources/secondary\",\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyPDFLoader,\n",
    ")\n",
    "secondary_docs = secondary_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_splits = text_splitter.split_documents(secondary_docs)\n",
    "print(f\"Split secondary documents into {len(secondary_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "def contains_nul(obj: Any) -> bool:\n",
    "    \"\"\"\n",
    "    True if `obj`, once JSON-encoded, contains the escape \\\\u0000.\n",
    "    Handles nested dicts/lists/tuples/sets transparently.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dumped = json.dumps(obj, ensure_ascii=True)  # Postgres will escape non-ASCII anyway\n",
    "    except (TypeError, ValueError):\n",
    "        # json.dumps() will blow up on bytes – treat that as a fatal value.\n",
    "        return True\n",
    "    return \"\\\\u0000\" in dumped\n",
    "\n",
    "bad = []\n",
    "for i, d in enumerate(secondary_splits):\n",
    "    if contains_nul({\"page_content\": d.page_content, **d.metadata}):\n",
    "        bad.append(i)\n",
    "\n",
    "print(f\"NUL detected in {len(bad)} docs → {bad[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_vector_store = PGVector.from_documents(\n",
    "    documents=secondary_splits,                 # your list of split Documents\n",
    "    embedding=embeddings,\n",
    "    connection=\"postgresql+psycopg://neondb_owner:npg_NkGRWx4yetz3@ep-odd-water-a7dn9f9o-pooler.ap-southeast-2.aws.neon.tech/neondb?sslmode=require\",\n",
    "    collection_name=\"nietzsche_secondary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    primary_context: List[Document]\n",
    "    secondary_context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State) -> State:\n",
    "    # you can tune k to the number of chunks you want from each\n",
    "    prim_hits = vector_store.similarity_search(state[\"question\"], k=3)\n",
    "    sec_hits  = secondary_vector_store.similarity_search(state[\"question\"], k=3)\n",
    "    return {\n",
    "        \"question\": state[\"question\"],\n",
    "        \"primary_context\": prim_hits,\n",
    "        \"secondary_context\": sec_hits,\n",
    "        \"answer\": \"\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 1. Write out exactly what you want your system message to say.\n",
    "template = \"\"\"\n",
    "You are zarabot-ai, a retrieval-augmented reAct chatbot specialized in Friedrich Nietzsche.\n",
    "When given a question\n",
    "Rules:\n",
    "- Answer using the retrieved chunks.\n",
    "- Quote primary source passages when relevant.\n",
    "- Cite or summarize secondary sources clearly.\n",
    "- If unsure, admit you don’t know. Do not hallucinate.\n",
    "\n",
    "Primary Source Chunks:\n",
    "{primary_context}\n",
    "\n",
    "Secondary Source Chunks:\n",
    "{secondary_context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Build the PromptTemplate\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def generate(state: State) -> State:\n",
    "    # flatten the Document.page_content to strings\n",
    "    prim_text = \"\\n\\n\".join(d.page_content for d in state[\"primary_context\"])\n",
    "    sec_text  = \"\\n\\n\".join(d.page_content for d in state[\"secondary_context\"])\n",
    "    messages = custom_rag_prompt.invoke({\n",
    "        \"primary_context\": prim_text,\n",
    "        \"secondary_context\": sec_text,\n",
    "        \"question\": state[\"question\"],\n",
    "    }).to_messages()\n",
    "    response = llm.invoke(messages)\n",
    "    return {**state, \"answer\": response.content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\"question\": \"explain the start of thus spoke Zarathustra when Zarathustra first comes down after his isolation\"})\n",
    "\n",
    "print(f'Context: {result[\"context\"]}\\n\\n')\n",
    "print(f'Answer: {result[\"answer\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
